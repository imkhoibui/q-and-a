An artificial neural network is made of connected units or nodes called artificial neurons, which loosely model the neurons in a brain. These are connected by edges, which model the synapses in a brain. An artificial neuron receives signals from connected neurons, then processes them and sends a signal to other connected neurons. The "signal" is a real number, and the output of each neuron is computed by some non-linear function of the sum of its inputs, called the activation function. Neurons and edges typically have a weight that adjusts as learning proceeds. The weight increases or decreases the strength of the signal at a connection.

Neural networks are typically trained through empirical risk minimization. This method is based on the idea of optimizing the network's parameters to minimize the difference, or empirical risk, between the predicted output and the actual target values in a given dataset. Gradient based methods such as backpropagation are usually used to estimate the parameters of the network. During the training phase, artificial neural networks learn from labeled training data by iteratively updating their parameters to minimize a defined loss function. This method allows the network to generalize to unseen data.

Historically, digital computers evolved from the von Neumann model, and operate via the execution of explicit instructions via access to memory by a number of processors. Neural networks, on the other hand, originated from efforts to model information processing in biological systems through the framework of connectionism. Unlike the von Neumann model, connectionist computing does not separate memory and processing.

The simplest kind of feedforward neural network (FNN) is a linear network, which consists of a single layer of output nodes; the inputs are fed directly to the outputs via a series of weights. The sum of the products of the weights and the inputs is calculated at each node. The mean squared errors between these calculated outputs and the given target values are minimized by creating an adjustment to the weights. This technique has been known for over two centuries as the method of least squares or linear regression. It was used as a means of finding a good rough linear fit to a set of points by Legendre (1805) and Gauss (1795) for the prediction of planetary movement.

Supervised learning uses a set of paired inputs and desired outputs. The learning task is to produce the desired output for each input. In this case, the cost function is related to eliminating incorrect deductions. A commonly used cost is the mean-squared error, which tries to minimize the average squared error between the network's output and the desired output. Tasks suited for supervised learning are pattern recognition (also known as classification) and regression (also known as function approximation). Supervised learning is also applicable to sequential data (e.g., for handwriting, speech and gesture recognition). This can be thought of as learning with a "teacher", in the form of a function that provides continuous feedback on the quality of solutions obtained thus far.

In natural language processing, ANNs are used for tasks such as text classification, sentiment analysis, and machine translation. They have enabled the development of models that can accurately translate between languages, understand the context and sentiment in textual data, and categorize text based on content. This has implications for automated customer service, content moderation, and language understanding technologies.

Natural language processing (NLP) is an interdisciplinary subfield of computer science and linguistics. It is primarily concerned with giving computers the ability to support and manipulate human language. It involves processing natural language datasets, such as text corpora or speech corpora, using either rule-based or probabilistic (i.e. statistical and, most recently, neural network-based) machine learning approaches. The goal is a computer capable of "understanding" the contents of documents, including the contextual nuances of the language within them. The technology can then accurately extract information and insights contained in the documents as well as categorize and organize the documents themselves. Challenges in natural language processing frequently involve speech recognition, natural-language understanding, and natural-language generation.

Up until the 1980s, most natural language processing systems were based on complex sets of hand-written rules. Starting in the late 1980s, however, there was a revolution in natural language processing with the introduction of machine learning algorithms for language processing. This was due to both the steady increase in computational power (see Moore's law) and the gradual lessening of the dominance of Chomskyan theories of linguistics (e.g. transformational grammar), whose theoretical underpinnings discouraged the sort of corpus linguistics that underlies the machine-learning approach to language processing.

Many of the notable early successes on statistical methods in NLP occurred in the field of machine translation, due especially to work at IBM Research, such as IBM alignment models. These systems were able to take advantage of existing multilingual textual corpora that had been produced by the Parliament of Canada and the European Union as a result of laws calling for the translation of all governmental proceedings into all official languages of the corresponding systems of government. However, most other systems depended on corpora specifically developed for the tasks implemented by these systems, which was (and often continues to be) a major limitation in the success of these systems. As a result, a great deal of research has gone into methods of more effectively learning from limited amounts of data.

With the growth of the web, increasing amounts of raw (unannotated) language data has become available since the mid-1990s. Research has thus increasingly focused on unsupervised and semi-supervised learning algorithms. Such algorithms can learn from data that has not been hand-annotated with the desired answers or using a combination of annotated and non-annotated data. Generally, this task is much more difficult than supervised learning, and typically produces less accurate results for a given amount of input data. However, there is an enormous amount of non-annotated data available (including, among other things, the entire content of the World Wide Web), which can often make up for the inferior results if the algorithm used has a low enough time complexity to be practical.

In 2010, Tomas Mikolov (then a PhD student at Brno University of Technology) with co-authors applied a simple recurrent neural network with a single hidden layer to language modelling, and in the following years he went on to develop Word2vec. In the 2010s, representation learning and deep neural network-style (featuring many hidden layers) machine learning methods became widespread in natural language processing. That popularity was due partly to a flurry of results showing that such techniques can achieve state-of-the-art results in many natural language tasks, e.g., in language modeling and parsing. This is increasingly important in medicine and healthcare, where NLP helps analyze notes and text in electronic health records that would otherwise be inaccessible for study when seeking to improve care or protect patient privacy.

Computer vision tasks include methods for acquiring, processing, analyzing and understanding digital images, and extraction of high-dimensional data from the real world in order to produce numerical or symbolic information, e.g. in the forms of decisions. Understanding in this context means the transformation of visual images (the input to the retina in the human analog) into descriptions of the world that make sense to thought processes and can elicit appropriate action. This image understanding can be seen as the disentangling of symbolic information from image data using models constructed with the aid of geometry, physics, statistics, and learning theory.

The organization of a computer vision system is highly application-dependent. Some systems are stand-alone applications that solve a specific measurement or detection problem, while others constitute a sub-system of a larger design which, for example, also contains sub-systems for control of mechanical actuators, planning, information databases, man-machine interfaces, etc. The specific implementation of a computer vision system also depends on whether its functionality is pre-specified or if some part of it can be learned or modified during operation. Many functions are unique to the application. There are, however, typical functions that are found in many computer vision systems.

Image restoration comes into the picture when the original image is degraded or damaged due to some external factors like lens wrong positioning, transmission interference, low lighting or motion blurs, etc., which is referred to as noise. When the images are degraded or damaged, the information to be extracted from them also gets damaged. Therefore we need to recover or restore the image as it was intended to be. The aim of image restoration is the removal of noise (sensor noise, motion blur, etc.) from images. The simplest possible approach for noise removal is various types of filters, such as low-pass filters or median filters. More sophisticated methods assume a model of how the local image structures look to distinguish them from noise. By first analyzing the image data in terms of the local image structures, such as lines or edges, and then controlling the filtering based on local information from the analysis step, a better level of noise removal is usually obtained compared to the simpler approaches.

Currently, the best algorithms for such tasks are based on convolutional neural networks. An illustration of their capabilities is given by the ImageNet Large Scale Visual Recognition Challenge; this is a benchmark in object classification and detection, with millions of images and 1000 object classes used in the competition. Performance of convolutional neural networks on the ImageNet tests is now close to that of humans. The best algorithms still struggle with objects that are small or thin, such as a small ant on the stem of a flower or a person holding a quill in their hand. They also have trouble with images that have been distorted with filters (an increasingly common phenomenon with modern digital cameras). By contrast, those kinds of images rarely trouble humans. Humans, however, tend to have trouble with other issues. For example, they are not good at classifying objects into fine-grained classes, such as the particular breed of dog or species of bird, whereas convolutional neural networks handle this with ease.

The Fulbright Program, including the Fulbright–Hays Program, is one of several United States Cultural Exchange Programs with the goal of improving intercultural relations, cultural diplomacy, and intercultural competence between the people of the United States and other countries through the exchange of persons, knowledge, and skills. Via the program, competitively-selected American citizens including students, scholars, teachers, professionals, scientists, and artists may receive scholarships or grants to study, conduct research, teach, or exercise their talents abroad; and citizens of other countries may qualify to do the same in the United States.

The Fulbright Program is administered by cooperating organizations such as the Institute of International Education and operates in over 160 countries around the world. The Bureau of Educational and Cultural Affairs of the U.S. Department of State sponsors the Fulbright Program and receives funding from the United States Congress via annual appropriation bills. Additional direct and in-kind support comes from partner governments, foundations, corporations, and host institutions both in and outside the U.S. In 49 countries, a bi-national Fulbright Commission administers and oversees the Fulbright Program. In countries that have an active program but no Fulbright Commission, the Public Affairs Section of the U.S. Embassy oversees the Fulbright Program. More than 370,000 people have participated in the program since it began; 62 Fulbright alumni have won Nobel Prizes; 88 have won Pulitzer Prizes.

In 1945, Senator J. William Fulbright proposed a bill to use the proceeds from selling surplus U.S. government war property to fund international exchange between the U.S. and other countries. With the crucial timing of the aftermath of the Second World War and with the pressing establishment of the United Nations, the Fulbright Program was an attempt to promote peace and understanding through educational exchange. The bill devised a plan to forgo the debts foreign countries amassed during the war in return for funding an international educational program. It was through the belief that this program would be an essential vehicle to promote peace and mutual understanding between individuals, institutions and future leaders wherever they may be.

Vietnam has an extensive state-controlled network of schools, colleges, and universities and a growing number of privately run and partially privatised institutions. General education in Vietnam is divided into five categories: kindergarten, elementary schools, middle schools, high schools, and universities. A large number of public schools have been constructed across the country to raise the national literacy rate, which stood at 90% in 2008. Most universities are located in major cities of Hanoi and Ho Chi Minh City with the country's education system continuously undergoing a series of reforms by the government. Basic education in the country is relatively free for the poor although some families may still have trouble paying tuition fees for their children without some form of public or private assistance. Regardless, Vietnam's school enrolment is among the highest in the world. The number of colleges and universities increased dramatically in the 2000s from 178 in 2000 to 299 in 2005. In higher education, the government provides subsidised loans for students through the national bank, although there are deep concerns about access to the loans as well the burden on students to repay them. Since 1995, enrolment in higher education has grown tenfold to over 2.2 million with 84,000 lecturers and 419 institutions of higher education. A number of foreign universities operate private campuses in Vietnam, including Harvard University (United States) and the Royal Melbourne Institute of Technology (Australia). The government's strong commitment to education has fostered significant growth but still need to be sustained to retain academics. In 2018, a decree on university autonomy allowing them to operate independently without ministerial control is in its final stages of approval. The government will continue investing in education especially for the poor to have access to basic education.

Vietnam went through prolonged warfare in the 20th century. After World War II, France returned to reclaim colonial power in the First Indochina War, from which Vietnam emerged victorious in 1954. As a result of the treaties signed between the Viet Minh and France, Vietnam was also separated into two parts. The Vietnam War began shortly after, between the communist North Vietnam, supported by the Soviet Union and China, and the anti-communist South Vietnam, supported by the United States. Upon the North Vietnamese victory in 1975, Vietnam reunified as a unitary socialist state under the Communist Party of Vietnam (CPV) in 1976. An ineffective planned economy, a trade embargo by the West, and wars with Cambodia and China crippled the country further. In 1986, the CPV initiated economic and political reforms similar to the Chinese economic reform, transforming the country to a socialist-oriented market economy. The reforms facilitated Vietnamese reintegration into the global economy and politics.

Archaeological excavations have revealed the existence of humans in what is now Vietnam as early as the Paleolithic age. Stone artefacts excavated in Gia Lai province have been claimed to date to 0.78 Ma, based on associated find of tektites, however this claim has been challenged because tektites are often found in archaeological sites of various ages in Vietnam. Homo erectus fossils dating to around 500,000 BC have been found in caves in Lang Son and Nghe An provinces in northern Vietnam. The oldest Homo sapiens fossils from mainland Southeast Asia are of Middle Pleistocene provenance, and include isolated tooth fragments from Tham Om and Hang Hum. Teeth attributed to Homo sapiens from the Late Pleistocene have been found at Dong Can, and from the Early Holocene at Mai Da Dieu, Lang Gao and Lang Cuom. Areas comprising what is now Vietnam participated in the Maritime Jade Road, as ascertained by archeological research.

In 2010, Vietnam's total state spending on science and technology amounted to roughly 0.45% of its GDP. Vietnamese scientists have made many significant contributions in various fields of study, most notably in mathematics. Hoang Tuy pioneered the applied mathematics field of global optimisation in the 20th century, while Ngo Bao Chau won the 2010 Fields Medal for his proof of fundamental lemma in the theory of automorphic forms. Since the establishment of the Vietnam Academy of Science and Technology (VAST) by the government in 1975, the country is working to develop its first national space flight program especially after the completion of the infrastructure at the Vietnam Space Centre (VSC) in 2018. Vietnam has also made significant advances in the development of robots, such as the TOPIO humanoid model. One of Vietnam's main messaging apps, Zalo, was developed by Vuong Quang Khai, a Vietnamese hacker who later worked with the country's largest information technology service company, the FPT Group.

Throughout the history of Vietnam, its economy has been based largely on agriculture—primarily wet rice cultivation. Bauxite, an important material in the production of aluminium, is mined in central Vietnam. Since reunification, the country's economy is shaped primarily by the CPV through Five Year Plans decided upon at the plenary sessions of the Central Committee and national congresses. The collectivisation of farms, factories, and capital goods was carried out as part of the establishment of central planning, with millions of people working for state enterprises. Under strict state control, Vietnam's economy continued to be plagued by inefficiency, corruption in state-owned enterprises, poor quality and underproduction. With the decline in economic aid from its main trading partner, the Soviet Union, following the erosion of the Eastern bloc in the late 1980s, and the subsequent collapse of the Soviet Union, as well as the negative impacts of the post-war trade embargo imposed by the United States, Vietnam began to liberalise its trade by devaluing its exchange rate to increase exports and embarked on a policy of economic development.

As a result of several land reform measures, Vietnam has become a major exporter of agricultural products. It is now the world's largest producer of cashew nuts, with a one-third global share; the largest producer of black pepper, accounting for one-third of the world's market; and the second-largest rice exporter in the world after Thailand since the 1990s. Subsequently, Vietnam is also the world's second largest exporter of coffee. The country has the highest proportion of land use for permanent crops together with other states in the Greater Mekong Subregion. Other primary exports include tea, rubber and fishery products. Agriculture's share of Vietnam's GDP has fallen in recent decades, declining from 42% in 1989 to 20% in 2006 as production in other sectors of the economy has risen.

In 1986, the Sixth National Congress of the CPV introduced socialist-oriented market economic reforms as part of the Đoi Moi reform program. Private ownership began to be encouraged in industry, commerce and agriculture and state enterprises were restructured to operate under market constraints. This led to the five-year economic plans being replaced by the socialist-oriented market mechanism. As a result of these reforms, Vietnam achieved approximately 8% annual gross domestic product (GDP) growth between 1990 and 1997. The United States ended its economic embargo against Vietnam in early 1994. Although the 1997 Asian financial crisis caused an economic slowdown to 4–5% growth per year, its economy began to recover in 1999, and grew at around 7% per year from 2000 to 2005, one of the fastest in the world. On 11 January 2007, Vietnam became the 150th member of the WTO (World Trade Organization). According to the General Statistics Office of Vietnam (GSO), growth remained strong despite the late-2000s global recession, holding at 6.8% in 2010. Vietnam's year-on-year inflation rate reached 11.8% in December 2010 and the currency, the Vietnamese đồng, was devalued three times.